{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38162ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = './hugging_face_models/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47786e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224fb4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset\n",
    "\n",
    "df = pd.read_csv('./data/imdb/imdb_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e906e45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of records are 50000\n",
      "Total no of unique labels are 2 with values ['positive' 'negative']\n",
      "The distribution of output labels are positive    0.5\n",
      "negative    0.5\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting data statistics\n",
    "\n",
    "print(f\"Total no of records are {df.shape[0]}\")\n",
    "\n",
    "print(f\"Total no of unique labels are {df['sentiment'].nunique()} with values {df['sentiment'].unique()}\")\n",
    "\n",
    "print(f\"The distribution of output labels are {df['sentiment'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fb620d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting output label to numeric\n",
    "\n",
    "label_mapper = {'positive':1,'negative':0}\n",
    "\n",
    "df['sentiment'] = df['sentiment'].map(label_mapper)\n",
    "\n",
    "df['sentiment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a5426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text column\n",
    "\n",
    "def preprocess_reviews(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9.!?&\\'*]\",\" \",text)\n",
    "    text = re.sub(r\"(\\s)+\",\" \",text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['review_clean'] = df['review'].apply(preprocess_reviews)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055746d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>a wonderful little production. br br the filmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "      <td>petter mattei's love in the time of money is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...          1   \n",
       "1  A wonderful little production. <br /><br />The...          1   \n",
       "2  I thought this was a wonderful way to spend ti...          1   \n",
       "3  Basically there's a family where a little boy ...          0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1   \n",
       "\n",
       "                                        review_clean  \n",
       "0  one of the other reviewers has mentioned that ...  \n",
       "1  a wonderful little production. br br the filmi...  \n",
       "2  i thought this was a wonderful way to spend ti...  \n",
       "3  basically there's a family where a little boy ...  \n",
       "4  petter mattei's love in the time of money is a...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb8b1a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37500, 2) (37500,)\n",
      "(12500, 2) (12500,)\n"
     ]
    }
   ],
   "source": [
    "# splitting into train,val\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(df.drop('sentiment',axis=1),df['sentiment'],test_size=0.25,stratify=df['sentiment'],random_state=60)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6b5f2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up GPU support\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f06cc86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c600e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to the format bert expects\n",
    "\n",
    "def return_bert_inputs(df,text_column,tokenizer):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in df[text_column]:\n",
    "        \n",
    "        res_dict = tokenizer.encode_plus(text,add_special_tokens=True,max_length=512,\n",
    "                                          pad_to_max_length=True,return_attention_mask=True)\n",
    "        \n",
    "        input_ids.append(res_dict['input_ids'])\n",
    "        attention_masks.append(res_dict['attention_mask'])\n",
    "    \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids,attention_masks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50229fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\saura\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37500, 512]) torch.Size([37500, 512])\n",
      "torch.Size([12500, 512]) torch.Size([12500, 512])\n"
     ]
    }
   ],
   "source": [
    "tr_input_ids,tr_attention_masks = return_bert_inputs(X_train,'review_clean',bert_tokenizer)\n",
    "\n",
    "val_input_ids,val_attention_masks = return_bert_inputs(X_val,'review_clean',bert_tokenizer)\n",
    "\n",
    "print(tr_input_ids.shape,tr_attention_masks.shape)\n",
    "print(val_input_ids.shape,val_attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ca87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating appropriate data loaders for training\n",
    "\n",
    "\n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_val = torch.tensor(y_val.values)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(tr_input_ids,tr_attention_masks,y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "\n",
    "val_data = TensorDataset(val_input_ids,val_attention_masks,y_val)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_loader = DataLoader(val_data,sampler=val_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97701988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101,  1045,  2064,  ...,     0,     0,     0],\n",
       "         [  101,  2023,  2003,  ...,     0,     0,     0],\n",
       "         [  101,  2882,  8680,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2065,  2017,  ...,     0,     0,     0],\n",
       "         [  101,  2023,  2143,  ...,     0,     0,     0],\n",
       "         [  101, 27178, 18223,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "         0, 0, 1, 0, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at one batch of data loader\n",
    "\n",
    "iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba2eea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our custom model\n",
    "\n",
    "class CustomBert(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_dim,output_shape):\n",
    "        super(CustomBert,self).__init__()\n",
    "        \n",
    "        bert_input = 768\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        self.bert = bert_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(bert_input,self.hidden_dim),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(self.hidden_dim,self.output_shape))\n",
    "        \n",
    "        # Freezing bert\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self,input_ids,attention_masks):\n",
    "        x = self.bert(input_ids,attention_masks)[0][:,0,:] # getting [cls] token embeddings only\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea511ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBert(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model = CustomBert(64,2).to(device)\n",
    "custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e35b38a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, step per epoch : 0, loss : 0.6820238828659058\n",
      "Epoch : 1, step per epoch : 100, loss : 0.6959987878799438\n",
      "Epoch : 1, step per epoch : 200, loss : 0.6901320219039917\n",
      "Epoch : 1, step per epoch : 300, loss : 0.6916142106056213\n",
      "Epoch : 1, step per epoch : 400, loss : 0.6882206797599792\n",
      "Epoch : 1, step per epoch : 500, loss : 0.6643081903457642\n",
      "Epoch : 1, step per epoch : 600, loss : 0.6945164203643799\n",
      "Epoch : 1, step per epoch : 700, loss : 0.6791486144065857\n",
      "Epoch : 1, step per epoch : 800, loss : 0.6945682764053345\n",
      "Epoch : 1, step per epoch : 900, loss : 0.6641589999198914\n",
      "Epoch : 1, step per epoch : 1000, loss : 0.671757698059082\n",
      "Epoch : 1, step per epoch : 1100, loss : 0.6458482146263123\n"
     ]
    }
   ],
   "source": [
    "## Training loop\n",
    "\n",
    "epochs = 1\n",
    "total_steps_per_epochs = len(train_loader)\n",
    "\n",
    "optimizer = torch.optim.SGD(custom_model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    \n",
    "    custom_model.train() # put model in train mode\n",
    "    \n",
    "    for i,batch in enumerate(train_loader):\n",
    "        \n",
    "        b_ids,b_masks,b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        preds_b = custom_model(b_ids,b_masks)\n",
    "        \n",
    "        loss = criterion(preds_b,b_labels)\n",
    "        \n",
    "        optimizer.zero_grad() # reset any previous stored gradients\n",
    "        loss.backward() # do backprop\n",
    "        optimizer.step() # update weights\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch : {epoch}, step per epoch : {i}, loss : {loss}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78c87b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy is 0.741\n",
      "Accuracy of class 0 is 0.69504\n",
      "Accuracy of class 1 is 0.78704\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "\n",
    "custom_model.eval() # put model on eval mode\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "class_wise_correct = [0 for i in range(2)]\n",
    "class_wise_samples = [0 for i in range(2)]\n",
    "\n",
    "with torch.no_grad(): # no computation of gradients needed since we are just predicting\n",
    "    for batch in val_loader:\n",
    "        v_ids,v_masks,v_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        v_preds = custom_model(v_ids,v_masks)\n",
    "        _,v_preds = torch.max(v_preds,axis=1)\n",
    "        \n",
    "        total_correct += (v_preds == v_labels).sum().item()\n",
    "        total_samples += len(v_labels)\n",
    "        \n",
    "        for label,pred in zip(v_labels,v_preds):\n",
    "            \n",
    "            if label == pred:\n",
    "                class_wise_correct[label] += 1\n",
    "            \n",
    "            class_wise_samples[label] += 1\n",
    "\n",
    "print(f\"Overall accuracy is {total_correct/total_samples:.3f}\")\n",
    "for i in range(2):\n",
    "    print(f'Accuracy of class {i} is {class_wise_correct[i]/class_wise_samples[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
